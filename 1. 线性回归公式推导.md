## 线性回归公式推导

==推导思路（求解b）==

1. 最小二乘法导出损失函数$E(w,b)$ 

2. 证明损失函数$E(w,b)$是关于$w$和$b$的凸函数

3. 对损失函数$E(w,b)$求一阶偏导$\frac{\partial E(w,b)}{\partial b}$ 

4. 令一阶导数为$0$求解$b$

### 最小二乘法导出损失函数$E(w,b)$


$$
\begin{align}
E(w,b) 	&=\sum_{i=0}^m{(y_i-f(x_i))^2}\\
		&=\sum_{i=0}^m{(y_i-(wx_i+b))^2}\\
		&=\sum_{i=0}^m{(y_i-wx_i-b)^2}
\end{align}
$$

### 证明损失函数$E(w,b)$是关于$w$和$b$的凸函数

#### 二元函数凹凸性证明

$$
A=f^{''}_{xx}(x,y)\\
B=f^{''}_{xy}(x,y)\\
C=f^{''}_{yy}(x,y)\\
$$

- 在$D$上有$A>0$，$AC-B^2\geq0$，恒成立$\Rightarrow$凸函数；
- 在$D$上有$A<0$，$AC-B^2\geq0$，恒成立$\Rightarrow$凹函数。

$$
最值：对于(x_0,y_0) \in D\\
f^{'}_x(x_0,y_0)=0,且f^{'}_y(x_0,y_0)=0,\\
\Rightarrow f(x_0,y_0)为最大值或最小值
$$

#### 证明过程

$$
\begin{align}\frac{\partial E(w,b)}{\partial w} 	&=\frac{\partial}{\partial w}(\sum_{i=1}^m(y_i-wx_i-b)^2)\\									&=\sum_{i=1}^m \frac{\partial}{\partial w}(y_i-wx_i-b)^2)\\								&=\sum_{i=1}^m (2(y_i-wx_i-b)x_i) \tag{3.5}\\\end{align}
$$

$$
\begin{align}
A &=\frac{\partial^2 E(w,b)}{\partial^2 w}\\
&=\frac{\partial}{\partial w}(2(w\sum_{i=1}^m (x_i^2)-\sum_{i=1}^m (y_i-b)x_i)\\	
&=2\sum_{i=1}^mx_i^2 \geq 0
\end{align}
$$

$$
\begin{align}
B &=\frac{\partial^2 E(w,b)}{\partial w\partial b}\\
&=\frac{\partial}{\partial b}(2(w\sum_{i=1}^m (x_i^2)-\sum_{i=1}^m (y_i-b)x_i)\\	
&=2(-\sum_{i=1}^m(-x_i)=2\sum_{i=1}^mx_i
\end{align}
$$

$$
\begin{align}
C &=\frac{\partial^2 E(w,b)}{\partial^2 b}\\
&=\frac{\partial}{\partial b}[\sum_{i=1}^m2(y_i-wx_i-b)\cdot(-1)]\\	
&=\frac{\partial}{\partial b}\sum_{i=1}^m2(-y_i+wx_i+b)\\
&=\frac{\partial}{\partial b}2(mb-\sum_{i=1}^m(y_i-wx_i)) \tag{3.6}
\end{align}
$$

$$
\begin{align}
AC-B^2 &=2m\cdot2\sum_{i=1}^mx_i^2-(2\sum_{i=1}^mx_i)^2\\
&=2m\sum_{i=1}^mx_i^2-4(\sum_{i=1}^mx_i)^2\\
&=4m\sum_{i=1}^mx_i^2-4m\cdot\frac{1}{m}(\sum_{i=1}^mx_i)^2\\
&=4m\sum_{i=1}^mx_i^2-4m\cdot \overline{x}\sum_{i=1}^mx_i\\
&=4m(\sum_{i=1}^mx_i^2-\sum_{i=1}^mx_i\overline{x})\\
&=4m\sum_{i=1}^m(x_i^2-x_i\overline{x})
\end{align}\\
$$

其中，
$$
\begin{align}
\sum_{i=1}^mx_i\overline{x} &=m\cdot \overline{x}\cdot \frac{1}{m}\sum_{i=1}^mx_i\\
&=m\cdot\overline{x}^2=\sum_{i=1}^m\overline{x}^2
\end{align}
$$
所以，
$$
\begin{align}
AC-B^2 &=4m\sum_{i=1}^m(x_i^2-x_i\overline{x})\\
&=4m\cdot\sum_{i=1}^m(x_i^2-\overline{x}x_i+\overline{x}x_i-\overline{x}x_i)\\
&=4m\cdot\sum_{i=1}^m(x_i^2-2\overline{x}x_i+\overline{x}^2)\\
&=4m\sum_{i=1}^m(x_i-\overline{x})^2 \geq0 恒成立
\end{align}\\
$$
即，$E(w,b)$为凸函数

###对损失函数$E(w,b)$求一阶偏导$\frac{\partial E(w,b)}{\partial b}$ 令一阶导数为$0$求解$b$

$$
\frac{\partial}{\partial b}E(w,b)=2(mb-\sum_{i=1}^m(y_i-wx_i))=0\\

\begin{align}
\Rightarrow b &=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i) \tag{3.8}\\
&=\frac{1}{m}\sum_{i=1}^my_i-w\cdot\frac{1}{m}\sum_{i=1}^mx_i\\
&=\overline{y}-w\overline{x}
\end{align}
$$



### 对损失函数$E(w,b)$求一阶偏导$\frac{\partial E(w,b)}{\partial w}$ 令一阶导数为$0$求解$w$

$$
\frac{\partial}{\partial w}E(w,b)=2(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i=0\\
\Rightarrow w=\frac{\sum_{i=1}^my_ix_i-\sum_{i=1}^mbx_i}{\sum_{i=1}^mx_i^2}\\
\begin{align}
\Rightarrow w\sum_{i=1}^mx_i^2 &=\sum_{i=1}^my_ix_i-\sum_{i=1}^m(\overline{y}-w\overline{x})x_i\\
&=\sum_{i=1}^my_ix_i-\sum_{i=1}^m\overline{y}x_i-w\overline{x}\sum_{i=1}^mx_i\\
\end{align}\\
\Rightarrow w(\sum_{i=1}^mx_i^2+\overline{x}\sum_{i=1}^mx_i)=\sum_{i=1}^my_ix_i-\sum_{i=1}^m\overline{y}x_i\\
\Rightarrow w(\sum_{i=1}^mx_i^2+\frac{1}{m}(\sum_{i=1}^mx_i)^2)=\sum_{i=1}^my_ix_i-m\overline{y}\frac{1}{m}\sum_{i=1}^mx_i\\=\sum_{i=1}^my_ix_i-\overline{x}\sum_{i=1}^my_i\\
$$

$$
\Rightarrow w=\frac{\sum_{i=1}^my_i(x_i-\overline{x})}{\sum_{i=1}^mx_i^2+\frac{1}{m}(\sum_{i=1}^mx_i)^2} \tag{3.7}
$$

### *向量化操作

- 什么是向量化？

向量化是将$\Sigma$变成矩阵/向量点乘的形式

- 为什么要将$w$向量化操作？

python中不向量化操作就会使用for循环，向量化操作了之后可以使用numpy包进行快速计算。

- 如何将$w$向量化？

$$
\begin{align}
w &=\frac{\sum_{i=1}^my_i(x_i-\overline{x})}{\sum_{i=1}^mx_i^2+\frac{1}{m}(\sum_{i=1}^mx_i)^2}\\
&=\frac{\sum_{i=1}^my_i(x_i-\overline{x})}{\sum_{i=1}^mx_i^2+\sum_{i=1}^mx_i\overline{x}}\\
&=\frac{\sum_{i=1}^my_i(x_i-\overline{x})}{\sum_{i=1}^mx_i(x_i-\overline{x})}
\end{align}
$$

其中，
$$
\sum_{i=1}^my_i-\overline{x}=\sum_{i=1}^m\overline{y}-x_i=\sum_{i=1}^m\overline{x}\overline{y}\\
\sum_{i=1}^mx_i\overline{x}=\sum_{i=1}^m\overline{x}^2
x=(x_1,x_2,...,x_m)^T\\
x_d=(x_1-\overline{x},x_2-\overline{x},...,x_m-\overline{x})^T\\
y=(y_1,y_2,...,y_m)^T\\
y_d=(y_1-\overline{y},y_2-\overline{y},...,y_m-\overline{y})^T
$$
所以，
$$
\begin{align}
w &=\frac{\sum_{i=1}^m(y_ix_i-y_i\overline{x}-y_i\overline{x}+y_i\overline{x})}{\sum_{i=1}^m(x_i^2-x_i\overline{x}-x_i\overline{x}+x_i\overline{x})}\\
&=\frac{\sum_{i=1}^m(y_ix_i-y_i\overline{x}-\overline{y}x_i+\overline{x}\overline{y})}{\sum_{i=1}^m(x_i^2-2x_i\overline{x}+\overline{x}^2)}\\
&=\frac{\sum_{i=1}^m(y_i-\overline{y})(x_i-\overline{x})}{\sum_{i=1}^m(x_i-\overline{x})^2}\\
&=\frac{x_d^T\cdot y_d}{x_d^T\cdot x_d}
\end{align}
$$